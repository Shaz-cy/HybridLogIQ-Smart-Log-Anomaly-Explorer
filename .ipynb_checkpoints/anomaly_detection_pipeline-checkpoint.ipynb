{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d52278d-977a-4d6e-adc9-f68d02684319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFO instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFO instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFO instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFO instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFO 63543 double-hummer alignment exceptions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     log_message\n",
       "0  INFO instruction cache parity error corrected\n",
       "1  INFO instruction cache parity error corrected\n",
       "2  INFO instruction cache parity error corrected\n",
       "3  INFO instruction cache parity error corrected\n",
       "4  INFO 63543 double-hummer alignment exceptions"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Load your BGL log file ===\n",
    "with open(\"BGL_2k.log\", \"r\", encoding='utf-8', errors='ignore') as file:\n",
    "    logs = file.readlines()\n",
    "\n",
    "# === Optional: remove empty lines and trim whitespace ===\n",
    "logs = [line.strip() for line in logs if line.strip() != \"\"]\n",
    "\n",
    "# === Just keep the log message part (last few columns) ===\n",
    "# You can extract only the last part of the log using string split\n",
    "log_messages = [' '.join(line.split()[8:]) for line in logs]\n",
    "\n",
    "# === Create DataFrame ===\n",
    "df_logs = pd.DataFrame(log_messages, columns=[\"log_message\"])\n",
    "df_logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503f2605-d206-4312-850f-86474888e43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (2000, 768)\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "model.eval()\n",
    "\n",
    "# Embed function\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Apply to all log messages\n",
    "embeddings = []\n",
    "for i, msg in enumerate(df_logs[\"log_message\"]):\n",
    "    try:\n",
    "        emb = get_embedding(msg)\n",
    "        embeddings.append(emb)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        embeddings.append(np.zeros(768))  # fallback for failed embeddings\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"✅ Processed {i + 1} log messages\")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(\"✅ Done! Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897311e-0a63-4a1d-88b7-5e24f824d4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anomaly_detection]",
   "language": "python",
   "name": "conda-env-anomaly_detection-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
